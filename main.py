# -*- coding: utf-8 -*-
"""Copy of enterprise_rag_assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-oTo2R8KWtNFk8GOSk4x-bOINPgFWE8

**`Install the libraries`**

sentence-transformers ‚Üí for embeddings

faiss-cpu ‚Üí for similarity search

pypdf ‚Üí to read PDF

openai ‚Üí to call an LLM
"""

!pip install sentence-transformers faiss-cpu pypdf openai

"""**Set up the OpenAI API key**"""

import os

# üëá Replace the string with your real Groq key, keep it secret
os.environ["GROQ_API_KEY"] = "##################################################"

"""**Upload and read a PDF**"""

from google.colab import files
uploaded = files.upload()

from pypdf import PdfReader

pdf_path = list(uploaded.keys())[0]
reader = PdfReader(pdf_path)

all_text = ""
for page in reader.pages:
    text = page.extract_text()
    if text:
        all_text += text + "\n"

print(all_text[:1000])  # show the first 1000 characters

"""**Chunk the text**"""

def split_into_chunks(text, max_chars=800, overlap=200):
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    chunks = []
    current = ""

    for para in paragraphs:
        if len(current) + len(para) + 1 <= max_chars:
            current += " " + para
        else:
            if current:
                chunks.append(current.strip())
            # keep some overlap from the end of previous chunk
            current = current[-overlap:] + " " + para
    if current:
        chunks.append(current.strip())
    return chunks

chunks = split_into_chunks(all_text)
len(chunks), chunks[0][:400]

"""**Compute embeddings with Hugging Face (sentence-transformers)**"""

from sentence_transformers import SentenceTransformer
import numpy as np

embed_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embed_model = SentenceTransformer(embed_model_name)

chunk_embeddings = embed_model.encode(chunks, show_progress_bar=True)
chunk_embeddings = np.array(chunk_embeddings).astype("float32")
chunk_embeddings.shape

"""**Build a FAISS index**"""

import faiss

d = chunk_embeddings.shape[1]  # embedding dimension
index = faiss.IndexFlatL2(d)
index.add(chunk_embeddings)

print("Vectors in index:", index.ntotal)

"""**Write a retrieval + LLM answer function**

Helper: get top-k relevant chunks
"""

def retrieve_chunks(query, k=5):
    query_emb = embed_model.encode([query]).astype("float32")
    distances, indices = index.search(query_emb, k)
    retrieved = [chunks[i] for i in indices[0]]
    return retrieved

"""Test it:"""

test_query = "What is this document mainly about?"
for i, c in enumerate(retrieve_chunks(test_query, k=3), start=1):
    print(f"--- Chunk {i} ---")
    print(c[:400], "\n")

"""Connect to Groq (Llama 3.1) as the LLM"""

!pip install groq

from groq import Groq

groq_client = Groq()  # uses GROQ_API_KEY from env

"""Build the answer function (RAG)
This function:

Retrieves relevant chunks

Builds a prompt with context

Calls Llama 3.1 via Groq

Returns the answer
"""

def answer_with_rag(query, k=5, model_name="llama-3.1-8b-instant"):
    # 1) Retrieve top-k chunks
    relevant_chunks = retrieve_chunks(query, k=k)
    context = "\n\n---\n\n".join(relevant_chunks)

    # 2) Build prompt
    prompt = f"""
You are an assistant that answers questions based ONLY on the context.

Context:
{context}

Question: {query}

Instructions:
- Use only the information in the context.
- If the answer is not clearly in the context, say you don't know.
- Be concise but clear.
"""

    # 3) Call Groq LLM
    chat_completion = groq_client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=512,
    )

    return chat_completion.choices[0].message.content.strip()

"""Ask real questions"""

print(answer_with_rag("What is the main focus of this document?", k=5))

!pip install pymupdf

import fitz  # PyMuPDF

doc = fitz.open(pdf_path)

all_text = ""
for page in doc:
    all_text += page.get_text() + "\n"

print(all_text[:1500])

print(answer_with_rag("What are Amazon's major business segments as listed in the report?", k=7))

print(answer_with_rag("What are Amazon‚Äôs primary revenue sources in this report?", k=7))

print(answer_with_rag("What are Amazon‚Äôs long-term strategic priorities?", k=7))

print(answer_with_rag("In the financial highlights, how does Amazon describe growth?", k=7))

"""Helper: generic LLM call"""

def call_llm(messages, model_name="llama-3.1-8b-instant", temperature=0.2, max_tokens=512):
    resp = groq_client.chat.completions.create(
        model=model_name,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp.choices[0].message.content.strip()

"""Define the tools (these are the agent‚Äôs ‚Äúactions‚Äù)"""

# Tool 1: search the document and return raw passages
def tool_search_docs(query, k=5):
    passages = retrieve_chunks(query, k=k)
    return "\n\n---\n\n".join(passages)

# Tool 2: use your existing RAG to directly answer
def tool_rag_answer(query, k=5):
    return answer_with_rag(query, k=k)

# Tool 3: generic summarizer (LLM-only)
def tool_summarize(text):
    prompt = f"Summarize the following text in 3‚Äì5 bullet points:\n\n{text}"
    return call_llm([
        {"role": "system", "content": "You are a concise summarization assistant."},
        {"role": "user", "content": prompt},
    ])

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# New advanced tools
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def tool_extract_numbers(text: str) -> str:
    """
    Extract key numeric facts (amounts, percentages, dates) from the given text.
    Returns a human-readable bullet list.
    """
    prompt = f"""
You are a financial data extraction assistant.

From the following text, extract all important numeric facts (revenues, growth rates, percentages, dates, counts, etc.).
For each fact, write one bullet point in this format:

- [Label]: <value> <unit if any> ‚Äì <very short context>

Only include facts that are clearly specified. Do not guess.

Text:
{text}
"""
    return call_llm(
        [
            {"role": "system", "content": "You extract numeric facts from text precisely and concisely."},
            {"role": "user", "content": prompt},
        ]
    )


def tool_compare_numbers(text: str) -> str:
    """
    Compare numeric facts in the given text.
    The text can contain raw passages and/or extracted numeric bullets.
    """
    prompt = f"""
You are a financial analyst.

The following text contains numeric facts (revenues, growth rates, segment figures, etc.).
1) Identify the main segments or categories.
2) Compare their values and growth (who is higher/lower, faster/slower, improving/declining).
3) Point out any notable trends or gaps (e.g., missing values for certain segments).

Write your answer as:
- Short comparison paragraph
- Then 3‚Äì5 concise bullet points of key comparisons.

Text:
{text}
"""
    return call_llm(
        [
            {"role": "system", "content": "You compare numeric business metrics and explain them clearly."},
            {"role": "user", "content": prompt},
        ]
    )


def tool_generate_insights(text: str) -> str:
    """
    Generate business insights combining numeric and qualitative information.
    The input should include the user question and key facts/segments.
    """
    prompt = f"""
You are a senior business analyst.

The text below contains:
- The user's question
- Relevant passages from a corporate report
- Possibly extracted numeric facts and comparisons

Your task:
1) Answer the user's question.
2) Highlight 3‚Äì5 key financial AND business insights (segments, risks, strategy, trends).
3) Mention any important uncertainties or missing data if relevant.

Be concise, structured, and grounded only in the given text.

Text:
{text}
"""
    return call_llm(
        [
            {"role": "system", "content": "You provide grounded financial and business insights."},
            {"role": "user", "content": prompt},
        ]
    )

"""register them in a dictionary:"""

TOOLS = {
    "search_docs": {
        "description": "Search the document and retrieve the most relevant passages.",
        "func": tool_search_docs,
    },
    "rag_answer": {
        "description": "Use the RAG pipeline to answer a question based on the document.",
        "func": tool_rag_answer,
    },
    "summarize": {
        "description": "Summarize or combine information from earlier observations.",
        "func": tool_summarize,
    },
    "extract_numbers": {
        "description": "Extract numeric facts (revenues, growth %, dates, counts) from text.",
        "func": tool_extract_numbers,
    },
    "compare_numbers": {
        "description": "Compare numeric facts across segments and highlight differences.",
        "func": tool_compare_numbers,
    },
    "generate_insights": {
        "description": "Generate financial and business insights based on facts and comparisons.",
        "func": tool_generate_insights,
    },
}

"""Agent system prompt (how it thinks)"""

import json

AGENT_SYSTEM_PROMPT = """
You are an advanced AI research agent that can use tools to answer questions about a document.

You have these tools:

1) search_docs(query):
   - Use this when you need to look up relevant passages from the document.
2) rag_answer(query):
   - Use this when you think the RAG pipeline can already answer the question directly.
3) summarize(text):
   - Use this to condense or combine information from previous observations.

You work in multiple steps:
- At each step you either call a tool OR give the final answer.
- Use search_docs first when you are unsure.
- Use summarize to combine a lot of raw text into something shorter.
- When you are confident, use final_answer.

VERY IMPORTANT:
You must respond ONLY in valid JSON, with this exact schema:

{
  "tool": "search_docs" | "rag_answer" | "summarize" | null,
  "input": "<what you want to send to the tool>",
  "final_answer": "<your final answer if you are done, otherwise an empty string>"
}

Rules:
- If you are NOT done, set final_answer to "" and choose a tool.
- If you ARE done, set tool to null and put the full user-facing final answer in final_answer.
- Do NOT add explanations, comments, or code fences. Only raw JSON.
"""

AGENT_SYSTEM_PROMPT = """
You are an advanced AI research agent that can use tools to answer questions about a long corporate document.

You have these tools:

1) search_docs(query):
   - Use this to look up relevant passages from the document.
2) rag_answer(query):
   - Use this when you think the RAG pipeline can already answer the question directly.
3) summarize(text):
   - Use this to condense or combine a lot of raw passages or observations.
4) extract_numbers(text):
   - Use this to extract key numeric facts (revenues, growth %, dates, counts) from passages or summaries.
5) compare_numbers(text):
   - Use this to compare numeric facts across segments (e.g., North America vs International) and highlight differences and trends.
6) generate_insights(text):
   - Use this to produce final financial AND business insights, grounded in the provided facts, for the user's question.

General strategy:
- Start with search_docs when you need context.
- If the question involves numbers, growth, performance, or segments, use extract_numbers and compare_numbers on the most relevant passages or summaries.
- Use summarize to keep information manageable when there is too much text.
- Use generate_insights near the end, when you have enough facts, to create a clear, grounded answer.
- Use rag_answer when a direct contextual answer is sufficient.

At each step you either call ONE tool OR, if you are done, give the final answer.

You must respond ONLY in valid JSON, with this schema:

{
  "tool": "search_docs" | "rag_answer" | "summarize" | "extract_numbers" | "compare_numbers" | "generate_insights" | null,
  "input": "<what you want to send to the tool>",
  "final_answer": "<your final answer if you are done, otherwise an empty string>"
}

Rules:
- If you are NOT done, set final_answer to "" and choose a tool.
- When calling tools:
   - For search_docs: input should be a focused query.
   - For summarize: input should be raw passages or previous observations.
   - For extract_numbers: input should be text that likely contains numeric facts.
   - For compare_numbers: input should include extracted numeric bullets and/or short passages.
   - For generate_insights: input should include the user's question and the key facts/comparisons you have gathered.
- If you ARE done, set tool to null and put the full user-facing final answer in final_answer.
- Do NOT add explanations or code fences. Only raw JSON.
"""

"""One agent step: let the LLM decide the next action"""

def agent_step(user_question, history, model_name="llama-3.1-8b-instant"):
    """
    history: list of dicts with keys: tool, input, observation
    """
    history_text = ""
    for i, step in enumerate(history, start=1):
        history_text += (
            f"Step {i}:\n"
            f"Tool: {step['tool']}\n"
            f"Input: {step['input']}\n"
            f"Observation: {step['observation']}\n\n"
        )

    user_content = f"""
User question:
{user_question}

Previous steps (if any):
{history_text}
Decide your next action.
"""

    raw = call_llm(
        [
            {"role": "system", "content": AGENT_SYSTEM_PROMPT},
            {"role": "user", "content": user_content},
        ],
        model_name=model_name,
        temperature=0.1,
        max_tokens=512,
    )

    # Try to parse JSON; if it fails, fall back to final_answer
    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        data = {"tool": None, "input": "", "final_answer": raw}

    # Ensure keys exist
    data.setdefault("tool", None)
    data.setdefault("input", "")
    data.setdefault("final_answer", "")

    return data

"""The agent loop (multi-step reasoning)"""

def run_agent(user_question, max_steps=5, model_name="llama-3.1-8b-instant"):
    history = []

    for step in range(max_steps):
        decision = agent_step(user_question, history, model_name=model_name)
        tool_name = decision.get("tool")
        tool_input = decision.get("input", "")
        final_answer = decision.get("final_answer", "")

        # If LLM produced a final answer, stop
        if tool_name is None or tool_name == "" or final_answer:
            if final_answer.strip() == "":
                # Ask model to summarize history into a final answer
                summary_prompt = "Based on the following tool calls and observations, answer the user's question:\n\n"
                for h in history:
                    summary_prompt += (
                        f"Tool: {h['tool']}\n"
                        f"Input: {h['input']}\n"
                        f"Observation: {h['observation']}\n\n"
                    )
                summary_prompt += f"\nUser question: {user_question}"
                final_answer = call_llm(
                    [
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": summary_prompt},
                    ],
                    model_name=model_name,
                )
            return final_answer, history

        # If tool is unknown, record and continue
        if tool_name not in TOOLS:
            observation = f"Unknown tool requested: {tool_name}"
        else:
            func = TOOLS[tool_name]["func"]
            # Limit observation length so prompt doesn't explode
            observation = func(tool_input)
            if isinstance(observation, str) and len(observation) > 1000:
                observation = observation[:1000] + "\n...[truncated]..."

            # ‚≠ê NEW RULE: If agent chooses generate_insights, immediately return final answer
            if tool_name == "generate_insights":
                # treat observation as final answer
                history.append(
                    {
                        "tool": tool_name,
                        "input": tool_input,
                        "observation": observation,
                    }
                )
                return observation, history

        # Save history after normal tool call
        history.append(
            {
                "tool": tool_name,
                "input": tool_input,
                "observation": observation,
            }
        )

    # If we hit max_steps without final answer, summarize
    summary_prompt = "You reached the step limit. Based on this history, answer the user's question:\n\n"
    for h in history:
        summary_prompt += (
            f"Tool: {h['tool']}\n"
            f"Input: {h['input']}\n"
            f"Observation: {h['observation']}\n\n"
        )
    summary_prompt += f"\nUser question: {user_question}"

    final_answer = call_llm(
        [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": summary_prompt},
        ],
        model_name=model_name,
    )

    return final_answer, history

"""Try the advanced agent"""

question = "Compare the performance of Amazon's main business segments and highlight key risks mentioned in the report."
final_answer, trace = run_agent(question, max_steps=5)

print("FINAL ANSWER:\n")
print(final_answer)
print("\n\nTRACE:\n")
for i, step in enumerate(trace, start=1):
    print(f"Step {i}: tool={step['tool']}")
    print("Input:", step['input'])
    print("Observation preview:", step['observation'][:200], "\n")

"""improve the reasoning"""

question = "Compare the financial performance of Amazon's main business segments and highlight key business risks mentioned in the report."
final_answer, trace = run_agent(question, max_steps=6)

print("FINAL ANSWER:\n")
print(final_answer)
print("\n\nTRACE:\n")
for i, step in enumerate(trace, start=1):
    print(f"Step {i}: tool={step['tool']}")
    print("Input preview:", step['input'][:120], "...\n")
    print("Observation preview:", step['observation'][:200], "\n")

# ========== FASTAPI SERVER FOR DEPLOYMENT ==========

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class Query(BaseModel):
    question: str

@app.post("/ask")
def ask_question(query: Query):
    # Call your existing agent function
    answer, trace = run_agent(query.question, max_steps=6)

    return {
        "answer": answer,
        "trace": trace
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)